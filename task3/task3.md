# Task3

### PyTorch基础实现代码
* 需要明确LR的机器学习三要素即 
1. 模型：广义的线性回归，实则为二分类问题，使用sigmoid非线性函数对线性组合的值进行映射为概率
2. 策略：目标函数：极大似然估计 
$$L(\theta) = - \sum_{i=1}^M(y^{(i)}log_{2}h_{\theta}(x^{(i)})+(1-y^{(i)})log_{2}(1-h_{\theta}(x^{(i)}))$$
3. 算法：梯度下降，sgd、bgd、adam，动量

### 用PyTorch类实现Logistic regression,torch.nn.module写网络结构
* 在代码中体现
1. 归一化 \
在机器学习领域中，不同的评价指标（即特征向量中的不同特征就是所述的不同评价指标）往往具有不同的量纲和量纲单位，这样的情况会影响到对数据的分析结果，为了消除指标之间的两个影响，需要进行数据标准化护理，以实现数据指标之间的可比性美元时数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价，其中典型的就是数据的归一化处理

2. 简而言之，归一化的目的就是使得预处理的数据被限定在一定的范围内（比如[0,1]或者[-1,1]）,从而消除奇异样本数据导致的不良的影响
    1) 在统计学中，归一化的具体作用是归纳统一样本的统计分布性，归一化在0～1之间是统计的概率分布，归一化在-1～+1之间是统计的坐标分布
    2) 奇异样本数据：是相对与其他输入样本特别大或特别小的样本矢量（即特征向量->列向量），奇异样本数据的存在会引起训练时间增大，同时也可能导致无法收敛，因此，当存在奇异样本数据时，在进行训练之前需要对预处理数据进行归一化；繁殖，不存在奇异样本数据时，则可以不进行归一化
3. 归一化后加快了梯度下降求最优解的速度\
   归一化有可能提高精度（如KNN）

4. 方式：\
    a) 最大最小标准化: \
    又称为离差标准化，使结棍映射到[0,1]之间，转换函数：
    $$X^{new}=\frac{X-min(X)}{max(X)-min(X)} $$
    其中$X$代表原来的数据中的一列数据即特征向量 \
    适用于数据比较集中的情况 \
    缺点：如果max和min不稳定，很容易使得归一化结果不稳。使得后续使用效果也不稳定。实际使用中可以用经验常量来替代max和min \
    应用场景：再不设计距离度量、协方差计算、数据不符合正态分布的时候，可以使用，比如图像处理中，将RGB图像转换为会读图像后将其限定在[0 255]的范围

    b) Z-score: \ 
    数据处理周符合标准正态分布，即均值为0，标准差为1，其转化函数为：
    $$X^{new}=\frac{X-\mu}{\sigma}$$ 
    其中$\mu$是数据样本某一列特征数据的均值， $\sigma$为数据样本某一列特征数据的标准差 \
    使用场景，要求原始数据的分布可以近似为高斯分布，否则归一化的效果会变得很糟糕 \
    应用场景：在分类、聚类算法中，需要使用距离来度量相似的时候，或者使用PCA技术进行降维的时候，Z-score表现更好


    c) 非线性归一化（例如sigmoid非线性函数的使用）
    本归一化方法经常用在数据分化比较大的情况下，有些数值很大，有一些很小，通过一些数学函数，将原始值进行映射。\
    该方法包括log，正切、L2范数归一化等，需要更具数据分布得情况，决定非线性函数的曲线